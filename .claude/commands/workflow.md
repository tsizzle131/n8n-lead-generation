---
description: Intelligently orchestrate all commands to complete tasks end-to-end
argument-hint: [task-description]
allowed-tools: Read, Write, Edit, Bash, Glob, Grep, TodoWrite, SlashCommand
model: claude-sonnet-4-5-20250929
---

# Workflow Meta-Command

You are the workflow orchestrator for Claude Code's command system. Your mission is to analyze a user's task, intelligently select and execute the optimal sequence of commands, and deliver comprehensive results with detailed reporting.

## Task Context

**User's Task**: {{ARGUMENTS}}

## Your Execution Framework

You will execute this task through a sophisticated multi-wave workflow that adapts to the task type. Follow this framework precisely:

---

## Wave 0: Context & Organization Check

**Purpose:** Read organization files to understand project context, active tasks, and strategic goals before planning workflow.

### Step 0.1: Read Organization Files

Attempt to read the following organization files (if they exist):

**Read TODO.md:**
- Look for active tasks `[â–¶]` related to this workflow task
- Identify blockers `[âš ]` that may affect execution
- Check recently completed tasks `[x]` for relevant context
- Note any related pending tasks

**Read STATUS.md:**
- Understand current project phase
- Check command availability and status
- Review recent work completed
- Check for any ongoing work that may conflict
- Review quality metrics baseline

**Read ROADMAP.md:**
- Identify current phase and objectives
- Understand strategic goals this workflow supports
- Check phase progress and next milestones
- Identify any phase dependencies

### Step 0.2: Extract Context Summary

Compile extracted information into a context summary:

```
Organization Context:
- Current Phase: [from ROADMAP or "Unknown"]
- Phase Progress: [X/Y objectives or "N/A"]
- Active Tasks: [list from TODO or "None found"]
- Blockers: [list from TODO or "None"]
- Recent Work: [from STATUS or "Unknown"]
- Related Work: [any similar completed tasks]
- Strategic Alignment: [which ROADMAP objective this supports]
- Quality Baseline: [current metrics from STATUS]
```

### Step 0.3: Apply Context to Workflow Planning

Use the context summary to inform execution:
- **If related work exists**: Reference patterns and approaches used
- **If blockers exist**: Address blockers or note limitations
- **If phase goals clear**: Align command sequence with phase objectives
- **If quality metrics available**: Set improvement targets

### Step 0.4: Graceful Degradation

**If organization files don't exist:**
- Note: "No organization files found - proceeding without project context"
- Continue with standard workflow planning
- Recommend running `.claude/init.sh` to initialize tracking

**If files exist but are minimal:**
- Use whatever context is available
- Proceed normally

---

## Wave 1: Task Analysis & Workflow Planning

### Step 1.1: Parse and Understand the Task

Analyze the user's task: `{{ARGUMENTS}}`

Extract the following:
- **Intent**: What does the user want to accomplish?
- **Target Path/Scope**: Which files, directories, or components are involved?
- **Key Entities**: Specific features, bugs, components mentioned
- **Constraints**: Any specific requirements, technologies, or limitations mentioned

### Step 1.2: Classify the Task Type

Use keyword detection to classify the task. Tasks can match multiple categories (hybrid tasks):

**Feature Development** - Keywords: implement, add, create, build, new, feature, develop
- Indicates: Building something new that doesn't exist yet
- Priority: Research â†’ Build â†’ Test â†’ Quality â†’ Document

**Bug Fix** - Keywords: fix, bug, error, issue, broken, crash, problem, failing
- Indicates: Something exists but doesn't work correctly
- Priority: Understand â†’ Find bugs â†’ Fix â†’ Test â†’ Document

**Quality Improvement** - Keywords: improve, refactor, clean, quality, debt, maintainability, readable
- Indicates: Code exists but needs improvement
- Priority: Find issues â†’ Refactor â†’ Test â†’ Document

**Performance Optimization** - Keywords: slow, optimize, faster, speed, performance, bottleneck, latency
- Indicates: Code works but needs to be faster/more efficient
- Priority: Analyze â†’ Optimize â†’ Test â†’ Verify

**Learning/Onboarding** - Keywords: explain, understand, how, what, docs, documentation, learn, onboard
- Indicates: User wants to understand existing code
- Priority: Explain â†’ Document â†’ No implementation

**Comprehensive** - Keywords: complete, full, end-to-end, everything, overhaul, rebuild, full-stack
- Indicates: Large-scale changes requiring all commands
- Priority: All commands in optimal sequence

**Hybrid Tasks**: Tasks can match multiple categories. For example:
- "fix and optimize the authentication system" â†’ Bug Fix + Performance
- "implement user registration with comprehensive tests" â†’ Feature + Quality
- "refactor and document the payment service" â†’ Quality + Learning

### Step 1.3: Determine Command Sequence

Based on task classification, select the optimal command pattern:

**Pattern 1 - New Feature Development:**
```
1. /research [feature-topic] â†’ Generate implementation plan
2. /build [plan-path] â†’ Implement the feature
3. /testgen [target-path] â†’ Generate comprehensive tests
4. /refactor [target-path] â†’ Ensure code quality
5. /optimize [target-path] â†’ Optimize performance
6. /document [target-path] â†’ Generate documentation
```

**Pattern 2 - Bug Fixing:**
```
1. /explain [target-path] â†’ Understand the context
2. /bughunter [target-path] â†’ Find all related bugs
3. /build [fix-plan] â†’ Implement fixes using bug findings
4. /testgen [target-path] â†’ Generate regression tests
5. /bughunter [target-path] â†’ Verify bugs are fixed
6. /document [target-path] â†’ Document the fix
```

**Pattern 3 - Code Quality Improvement:**
```
1. /bughunter [target-path] â†’ Find existing issues
2. /refactor [target-path] â†’ Improve code structure
3. /testgen [target-path] â†’ Increase test coverage
4. /optimize [target-path] â†’ Improve performance
5. /document [target-path] â†’ Update documentation
```

**Pattern 4 - Performance Optimization:**
```
1. /explain [target-path] â†’ Understand the code
2. /optimize [target-path] â†’ Fix bottlenecks
3. /testgen [target-path] â†’ Ensure correctness
4. /bughunter [target-path] â†’ Check for bugs introduced
5. /document [target-path] â†’ Document optimizations
```

**Pattern 5 - Learning/Onboarding:**
```
1. /explain [query] â†’ Deep explanation
2. /document [target-path] â†’ Generate comprehensive docs
```

**Pattern 6 - Comprehensive (All Commands):**
```
1. /research [topic] â†’ Research and plan
2. /explain [path] â†’ Understand existing code
3. /bughunter [path] â†’ Find all issues
4. /build [plan-path] â†’ Implement changes
5. /testgen [path] â†’ Generate tests
6. /refactor [path] â†’ Improve quality
7. /optimize [path] â†’ Optimize performance
8. /document [path] â†’ Generate documentation
```

**Pattern 7 - Hybrid Tasks:**
For hybrid tasks, intelligently merge patterns. For example:
- Bug Fix + Performance: Combine Pattern 2 and Pattern 4, avoiding duplicate commands
- Feature + Quality: Combine Pattern 1 and Pattern 3, ensuring quality from the start

### Step 1.4: Execution Mode Selection

Determine execution mode based on task scope:

**Quick Mode** (1-3 commands):
- Task is small, targeted, specific
- Single file or component
- Clear, simple objective
- Estimated duration: 5-10 minutes

**Standard Mode** (4-6 commands):
- Task is moderate complexity
- Multiple files or components
- Requires some quality assurance
- Estimated duration: 15-30 minutes

**Comprehensive Mode** (7-8 commands):
- Task is complex, large-scale
- Full system or major component
- Keywords: "complete", "comprehensive", "end-to-end", "overhaul"
- Estimated duration: 30-60+ minutes

**Custom Mode**:
- User explicitly specifies which commands to run
- Parse user's command list from task description

### Step 1.5: Display Execution Plan

Present the execution plan to the user in this format:

```markdown
# Workflow Execution Plan

## Task Analysis
**Original Task**: {{ARGUMENTS}}
**Task Type**: [Primary classification] + [Secondary if hybrid]
**Target Scope**: [Extracted path or "å…¨codebase"]
**Execution Mode**: [Quick/Standard/Comprehensive/Custom]
**Estimated Duration**: [X-Y minutes]

## Command Sequence
The following commands will be executed:

1. â¸ /research [topic] - Research and generate implementation plan
2. â¸ /build [plan-path] - Implement the feature
3. â¸ /testgen [target-path] - Generate comprehensive tests
4. â¸ /testgen [target-path] - Generate comprehensive tests
5. â¸ /refactor [target-path] - Improve code quality
6. â¸ /optimize [target-path] - Optimize performance
7. â¸ /document [target-path] - Generate documentation

**Legend**: â¸ Pending | â³ Running | âœ“ Complete | âœ— Failed | âŠ˜ Skipped

## Execution Strategy
[Brief explanation of why this sequence was chosen and what each phase will accomplish]

---

Proceeding with execution...
```

### Step 1.6: Initialize Execution Tracking

Create a tracking structure for the workflow execution:

```
Workflow Execution State:
- Start Time: [timestamp]
- Total Commands: [count]
- Commands Completed: 0
- Commands Failed: 0
- Commands Skipped: 0
- Current Phase: Planning Complete
- Overall Status: Starting
```

---

## Wave 1.5: Update Organization Files - Workflow Started

**Purpose:** Mark workflow as in-progress in organization files and record execution plan.

### Step 1.5.1: Update TODO.md (if exists)

Find and update TODO item:
- If TODO item exists for this task: Change `[ ]` to `[â–¶]` (mark as in progress)
- If no TODO item exists: Add new item:
  ```markdown
  ## ðŸ”¥ In Progress
  - [â–¶] [Task description] (Started: [timestamp])
    - Workflow: [execution mode] mode ([N] commands)
    - Phase: [current phase from ROADMAP]
    - Estimated: [duration]
  ```

### Step 1.5.2: Update STATUS.md (if exists)

Add to current work section:
```markdown
## ðŸ”„ Active Workflows
- **[Task description]** - In Progress
  - Started: [timestamp]
  - Mode: [Quick/Standard/Comprehensive]
  - Commands: [list of commands to execute]
  - Phase: [current phase]
  - Strategic Goal: [from ROADMAP context]
```

### Step 1.5.3: Record Context Used

Log the organization context that informed planning:
```
Context Applied to Planning:
- Used context from: [TODO/STATUS/ROADMAP or "None"]
- Phase alignment: [how workflow aligns with current phase]
- Related work referenced: [any prior similar work]
- Baseline metrics: [quality metrics from STATUS]
```

**Note:** If organization files don't exist, skip this wave silently.

---

## Wave 2: Research & Understanding Phase (with Validation)

### Step 2.1: Determine Research Need

Based on task type, decide if research/understanding is needed:

- **Feature Development** â†’ YES: Need /research to create plan
- **Bug Fix** â†’ YES: Need /explain to understand context
- **Quality Improvement** â†’ MAYBE: Use /explain if scope is unclear
- **Performance Optimization** â†’ YES: Need /explain to understand bottlenecks
- **Learning/Onboarding** â†’ YES: Core purpose is /explain
- **Comprehensive** â†’ YES: Need both /research and /explain

### Step 2.2: Execute Research Commands (with Validation Loop)

Initialize attempt tracking for this wave:
```
Research Phase Tracking:
- Command: [/research or /explain or both]
- Attempt: 1
- Max Attempts: 3
- Status: Starting
```

**For Feature Development Tasks:**

**Attempt Loop for /research:**

**ATTEMPT 1 (Initial Execution):**

Update status: "â³ Step 1/N: Running /research [topic] (Attempt 1/3)..."

Execute: `/research [extracted-topic-from-task]`

Wait for command completion. The /research command will:
- Search for similar implementations
- Generate a comprehensive implementation plan
- Save plan to `.claude/plans/research-[timestamp].md`

Capture the plan file path and extract:
- Key findings (3-5 bullet points)
- Recommended approach
- Files to create/modify
- Technologies/patterns to use

**VALIDATE RESEARCH OUTPUT:**

Check quality gates:
- âœ“ Plan file exists at `.claude/plans/research-[timestamp].md`?
- âœ“ Plan contains at least 3 sections (Objective, Requirements, Implementation)?
- âœ“ Research found at least 3 relevant files (check research summary)?
- âœ“ Plan has actionable tasks (not too vague)?

Calculate validation score (0-100):
- Plan file saved: 25 points
- Adequate structure: 25 points
- Found relevant files: 25 points
- Actionable tasks: 25 points

**IF VALIDATION SCORE >= 75:** Mark as âœ“ Passed, proceed to Step 2.3

**IF VALIDATION SCORE < 75 AND ATTEMPT < 3:**

**ATTEMPT 2 (Refined Strategy):**

Update status: "âš  Validation failed (score: [score]/100). Retrying with refined strategy... (Attempt 2/3)"

Analyze failure:
- If no files found: Broaden search scope, try alternative keywords
- If plan too vague: Add more context from task description
- If structure poor: Regenerate with explicit structure requirements

Refined execution strategy:
- Use broader search patterns (e.g., `**/*auth*` instead of `**/auth.js`)
- Include more file types (docs, configs, tests in addition to source)
- Search with case-insensitive patterns
- Look in common directories: `src/`, `lib/`, `app/`, `components/`

Execute: `/research [topic-with-refined-parameters]`

**VALIDATE AGAIN** (same criteria as Attempt 1)

**IF STILL FAILS AND ATTEMPT < 3:**

**ATTEMPT 3 (Fallback Strategy):**

Update status: "âš  Attempt 2 failed. Using fallback strategy... (Attempt 3/3)"

Fallback approach:
- Generate minimal viable plan based on task description alone
- Use best practices from knowledge base (no codebase search)
- Create basic structure: objective + requirements + skeleton implementation
- Add note: "âš  Limited codebase research - plan based on general best practices"

Execute: `/research [topic]` with fallback mode OR manually create basic plan

**VALIDATE AGAIN**

**IF STILL FAILS AFTER ATTEMPT 3:**
- Mark as "âš  NEEDS REVIEW - Research incomplete"
- Log failure: "Research could not find sufficient information after 3 attempts"
- Create minimal placeholder plan for human review
- Continue workflow (allow user to intervene)

**For Bug Fix/Performance/Learning Tasks:**

**Attempt Loop for /explain:**

**ATTEMPT 1 (Initial Execution):**

Update status: "â³ Step 1/N: Running /explain [target] (Attempt 1/3)..."

Execute: `/explain [extracted-target-path]`

Wait for command completion. The /explain command will:
- Analyze code structure
- Explain functionality
- Identify dependencies

Capture and extract:
- How the code works (summary)
- Key components (list)
- Potential problem areas (list)
- Dependencies (list)

**VALIDATE EXPLAIN OUTPUT:**

Check quality gates:
- âœ“ Target path was found and analyzed?
- âœ“ Generated explanation with at least 3 components identified?
- âœ“ Identified dependencies (even if none)?
- âœ“ No analysis errors or crashes?

Calculate validation score (0-100):
- Target analyzed: 30 points
- Components identified (â‰¥3): 30 points
- Dependencies identified: 20 points
- Clear explanation: 20 points

**IF VALIDATION SCORE >= 70:** Mark as âœ“ Passed, proceed to Step 2.3

**IF VALIDATION SCORE < 70 AND ATTEMPT < 3:**

**ATTEMPT 2 (Refined Strategy):**

Update status: "âš  Validation failed (score: [score]/100). Retrying with refined strategy... (Attempt 2/3)"

Analyze failure:
- If target not found: Try parent directory, look for similar named files
- If explanation shallow: Request deeper analysis, include related files
- If components missing: Expand scope to include imports/dependencies

Refined execution strategy:
- If specific file not found, explain the parent directory
- Include related files (imports, dependencies)
- Search for alternative implementations of same functionality
- Use broader path patterns

Execute: `/explain [refined-target-path]`

**VALIDATE AGAIN**

**IF STILL FAILS AND ATTEMPT < 3:**

**ATTEMPT 3 (Fallback Strategy):**

Update status: "âš  Attempt 2 failed. Using fallback strategy... (Attempt 3/3)"

Fallback approach:
- Explain based on available information (even if incomplete)
- Use file search to gather context about the target
- Generate explanation from code snippets found via Grep
- Add note: "âš  Limited analysis - target may not exist or path incorrect"

**VALIDATE AGAIN**

**IF STILL FAILS AFTER ATTEMPT 3:**
- Mark as "âš  NEEDS REVIEW - Explanation incomplete"
- Log failure: "Could not analyze target after 3 attempts"
- Provide what information was found
- Continue workflow with limited context

**For Comprehensive Tasks:**

Execute BOTH commands sequentially with validation:
1. First: `/research [topic]` (with validation loop as above)
2. Then: `/explain [path]` (with validation loop as above)

Combine outputs for complete understanding.

### Step 2.3: Store Validated Research Findings

Store all research findings in a structured format with validation data:

```
Research Phase Results:
- Command(s) Executed: [/research, /explain, or both]
- Attempts Required: [1-3]
- Validation Score: [0-100]
- Final Status: [âœ“ Passed / âš  Needs Review / âœ— Failed]
- Retry Strategy Used: [None / Refined / Fallback]
- Plan File: [path if /research was used]
- Key Findings: [list]
- Target Files: [list]
- Approach: [summary]
- Limitations: [any noted limitations from fallback]
- Duration: [total time across all attempts]
- Validation Details:
  * Quality Gates Passed: [X/4 for research or X/4 for explain]
  * Failure Reasons (if any): [list]
  * Retry Actions Taken: [description if retried]
```

Update execution status:
- Mark research command(s) status:
  * "âœ“ Complete" if validation passed (score â‰¥ 75 for research, â‰¥ 70 for explain)
  * "âš  Needs Review" if failed after 3 attempts
  * "âœ— Failed" if critical failure blocking workflow
- Update commands completed count
- Update total retry attempts count
- Log validation details for final report

### Step 2.4: Adaptive Path Adjustment

If research reveals unexpected information:
- Task is larger than expected â†’ Upgrade to Comprehensive mode
- Target path different than assumed â†’ Update all subsequent commands
- Dependencies found â†’ Add /explain for dependencies if needed
- No action needed â†’ Downgrade to Quick mode

---

## Wave 3: Issue Detection & Analysis Phase (with Validation)

### Step 3.1: Determine Bug Hunting Need

Based on task type and current findings:

- **Feature Development** â†’ SKIP: No existing code to analyze
- **Bug Fix** â†’ YES: Core objective is finding bugs
- **Quality Improvement** â†’ YES: Need to find issues to fix
- **Performance Optimization** â†’ MAYBE: Use if performance issues unclear
- **Learning/Onboarding** â†’ SKIP: Not relevant
- **Comprehensive** â†’ YES: Always hunt for bugs

### Step 3.2: Execute Bug Hunter (with Validation Loop)

If bug hunting is needed:

Initialize attempt tracking:
```
Bug Hunter Phase Tracking:
- Command: /bughunter
- Attempt: 1
- Max Attempts: 3
- Status: Starting
```

**ATTEMPT 1 (Initial Execution):**

Update status: "â³ Step 2/N: Running /bughunter [target] (Attempt 1/3)..."

Execute: `/bughunter [target-path-from-research]`

Wait for command completion. The /bughunter command will:
- Perform multi-wave analysis
- Identify bugs, code smells, security issues
- Generate prioritized bug report
- Save report to `.claude/bugs/bughunter-[timestamp].md`

**VALIDATE BUG HUNTER OUTPUT:**

Check quality gates:
- âœ“ Bug report file exists at `.claude/bugs/bughunter-[timestamp].md`?
- âœ“ Successfully analyzed target files (no analysis crashes)?
- âœ“ Report contains bug categorization (Critical/High/Medium/Low)?
- âœ“ Report has structured format with findings?

Calculate validation score (0-100):
- Report file saved: 30 points
- Target files analyzed successfully: 30 points
- Bug categorization present: 20 points
- Structured findings: 20 points

**IF VALIDATION SCORE >= 75:** Mark as âœ“ Passed, proceed to Step 3.3

**IF VALIDATION SCORE < 75 AND ATTEMPT < 3:**

**ATTEMPT 2 (Refined Strategy):**

Update status: "âš  Validation failed (score: [score]/100). Retrying with refined strategy... (Attempt 2/3)"

Analyze failure:
- If analysis crashed: Try smaller scope, analyze fewer files at once
- If target not found: Expand search to parent directory or similar paths
- If report incomplete: Re-run with more detailed analysis settings
- If no bugs found but expected issues: Broaden analysis criteria

Refined execution strategy:
- If full path failed, try parent directory or broader pattern
- Break large targets into smaller chunks
- Use more lenient analysis settings
- Include additional file types

Execute: `/bughunter [refined-target-path]`

**VALIDATE AGAIN**

**IF STILL FAILS AND ATTEMPT < 3:**

**ATTEMPT 3 (Fallback Strategy):**

Update status: "âš  Attempt 2 failed. Using fallback strategy... (Attempt 3/3)"

Fallback approach:
- Perform manual bug analysis using Read + Grep tools
- Check for common bug patterns via search:
  * Security: Search for SQL injection patterns, XSS vulnerabilities
  * Memory: Search for memory leaks, unclosed resources
  * Logic: Search for null checks, error handling
- Generate basic bug report manually
- Add note: "âš  Manual analysis - automated bug hunting encountered issues"

**VALIDATE AGAIN**

**IF STILL FAILS AFTER ATTEMPT 3:**
- Mark as "âš  NEEDS REVIEW - Bug analysis incomplete"
- Log failure: "Could not complete bug analysis after 3 attempts"
- Note: "Recommend manual code review"
- Continue workflow

### Step 3.3: Analyze Validated Bug Report

Read the generated bug report and extract:
- Total bugs found (count)
- Critical issues (list)
- High priority issues (list)
- Medium/Low priority issues (count)
- Security vulnerabilities (list)
- Performance bottlenecks (list)

Store in structured format with validation data:

```
Bug Analysis Results:
- Command Executed: /bughunter
- Attempts Required: [1-3]
- Validation Score: [0-100]
- Final Status: [âœ“ Passed / âš  Needs Review / âœ— Failed]
- Retry Strategy Used: [None / Refined / Fallback]
- Report File: [path]
- Total Bugs: [count]
- Critical: [count] - [list]
- High Priority: [count] - [list]
- Medium/Low: [count]
- Security Issues: [count] - [list]
- Performance Issues: [count] - [list]
- Limitations: [any noted from fallback]
- Duration: [total time across all attempts]
- Validation Details:
  * Quality Gates Passed: [X/4]
  * Failure Reasons (if any): [list]
  * Retry Actions Taken: [description if retried]
```

### Step 3.4: Prioritization and Planning

Based on bug report, determine:
- Which bugs MUST be fixed (critical/security)
- Which bugs SHOULD be fixed (high priority)
- Which bugs CAN be fixed (medium/low)
- Estimated effort for fixes

If bug count is very high (>20), consider:
- Focusing on critical/high only
- Warning user about scope expansion
- Offering to split into multiple workflows

Update execution status:
- Mark /bughunter as "âœ“ Complete"
- Update commands completed count
- If failed: Mark as "âœ— Failed", continue with partial info

---

## Wave 4: Implementation Phase (with Validation)

### Step 4.1: Determine Implementation Need

Based on task type:

- **Feature Development** â†’ YES: Core objective
- **Bug Fix** â†’ YES: Need to fix the bugs
- **Quality Improvement** â†’ MAYBE: Use /refactor instead if no new code
- **Performance Optimization** â†’ MAYBE: Use /optimize instead
- **Learning/Onboarding** â†’ SKIP: No implementation
- **Comprehensive** â†’ YES: Always implement changes

### Step 4.2: Prepare Build Input

Determine what to pass to /build command:

**For Feature Development:**
- Input: Research plan file path from Wave 2
- Command: `/build [plan-file-path]`

**For Bug Fixes:**
- Input: Create a build plan incorporating bug findings
- First: Create `.claude/plans/bugfix-[timestamp].md` with:
  - List of bugs to fix (from Wave 3)
  - Prioritized fix approach
  - Files to modify
  - Testing requirements
- Then: `/build [bugfix-plan-path]`

**For Comprehensive Tasks:**
- Input: Research plan enhanced with bug findings
- Combine research plan + bug report into comprehensive build plan
- Command: `/build [enhanced-plan-path]`

### Step 4.3: Execute Build Command (with Validation Loop)

Initialize attempt tracking:
```
Build Phase Tracking:
- Command: /build
- Attempt: 1
- Max Attempts: 3
- Status: Starting
```

**ATTEMPT 1 (Initial Execution):**

Update status: "â³ Step 3/N: Running /build [plan] (Attempt 1/3)..."

Execute: `/build [plan-file-path]`

Wait for command completion. The /build command will:
- Read the implementation plan
- Create/modify files as specified
- Implement features or fixes
- Follow best practices and patterns

**VALIDATE BUILD OUTPUT:**

Check quality gates:
- âœ“ All planned files created/modified successfully?
- âœ“ No syntax errors in generated code (check for parse errors)?
- âœ“ Code follows project conventions and patterns?
- âœ“ Implementation matches plan requirements?

Calculate validation score (0-100):
- All files created/modified: 30 points
- No syntax errors: 30 points
- Follows conventions: 20 points
- Matches plan requirements: 20 points

To check for syntax errors, use:
- Read generated files and look for obvious syntax issues
- If possible, run language-specific linter (e.g., `npm run lint`, `python -m py_compile`)
- Check build output for error messages

**IF VALIDATION SCORE >= 75:** Mark as âœ“ Passed, proceed to Step 4.4

**IF VALIDATION SCORE < 75 AND ATTEMPT < 3:**

**ATTEMPT 2 (Refined Strategy):**

Update status: "âš  Validation failed (score: [score]/100). Retrying with refined strategy... (Attempt 2/3)"

Analyze failure:
- If file creation failed: Check permissions, verify paths exist, create directories
- If syntax errors: Identify specific errors, fix and regenerate
- If doesn't match plan: Review plan more carefully, adjust implementation approach
- If conventions violated: Apply project-specific patterns more strictly

Refined execution strategy:
- Fix identified syntax errors before proceeding
- Create missing directories before file creation
- Apply stricter adherence to project patterns
- Break large implementations into smaller steps
- Review existing similar code for patterns

Execute: `/build [plan-file-path]` with corrections OR use Edit tool to fix specific issues

**VALIDATE AGAIN**

**IF STILL FAILS AND ATTEMPT < 3:**

**ATTEMPT 3 (Fallback Strategy):**

Update status: "âš  Attempt 2 failed. Using fallback strategy... (Attempt 3/3)"

Fallback approach:
- Implement minimal viable version (MVP)
- Focus on core functionality only
- Skip optional features temporarily
- Create file structure with placeholder implementations
- Add TODO comments for incomplete sections
- Note: "âš  Minimal implementation - may need refinement"

**VALIDATE AGAIN** (with lower threshold - 60% acceptable for fallback)

**IF STILL FAILS AFTER ATTEMPT 3:**
- Mark as "âš  NEEDS REVIEW - Implementation incomplete"
- Log failure: "Could not complete implementation after 3 attempts"
- List what was accomplished vs. what failed
- Provide partial implementation for manual completion
- Consider aborting workflow if this is critical

### Step 4.4: Capture Validated Build Results

After build completion, capture:
- Files created (list with paths)
- Files modified (list with paths)
- Lines of code added/changed
- Implementation summary
- Any warnings or errors
- Validation results

Store in structured format with validation data:

```
Implementation Results:
- Command Executed: /build
- Attempts Required: [1-3]
- Validation Score: [0-100]
- Final Status: [âœ“ Passed / âš  Needs Review / âœ— Failed]
- Retry Strategy Used: [None / Refined / Fallback]
- Input Plan: [path]
- Files Created: [count] - [list]
- Files Modified: [count] - [list]
- Lines Added: [count]
- Lines Modified: [count]
- Syntax Check: [âœ“ Clean / âš  Minor Issues / âœ— Errors]
- Summary: [description]
- Warnings: [list]
- Limitations: [any noted from fallback]
- Duration: [total time across all attempts]
- Validation Details:
  * Quality Gates Passed: [X/4]
  * Syntax Errors Fixed: [count]
  * Retry Actions Taken: [description if retried]
```

### Step 4.5: Alternative Implementation Paths

**If build is not appropriate:**

**For Quality Improvement:**
Skip to Wave 6 and use /refactor instead

**For Performance Optimization:**
Skip to Wave 6 and use /optimize instead

**For Non-Implementation Tasks:**
Mark implementation as "âŠ˜ Skipped - Not applicable"

Update execution status:
- Mark /build as "âœ“ Complete", "âœ— Failed", or "âŠ˜ Skipped"
- Update commands completed count
- If failed: Log error, decide if workflow can continue

---

## Wave 5: Testing Phase (with Validation)

### Step 5.1: Determine Testing Need

Testing is needed for all task types EXCEPT:
- **Learning/Onboarding** â†’ SKIP
- All others â†’ YES

### Step 5.2: Determine Test Target

Identify what needs testing:
- If new feature built: Test the new files
- If bug fixes applied: Test fixed files + regression tests
- If refactoring done: Test refactored components
- If optimization done: Test optimized code + performance

Extract target path from previous phase results:
- Use files created/modified in Wave 4
- Or use target path from Wave 1 if no implementation

### Step 5.3: Execute Test Generation (with Validation Loop)

Initialize attempt tracking:
```
Testing Phase Tracking:
- Command: /testgen
- Attempt: 1
- Max Attempts: 3
- Status: Starting
```

**ATTEMPT 1 (Initial Execution):**

Update status: "â³ Step 4/N: Running /testgen [target] (Attempt 1/3)..."

Execute: `/testgen [target-path]`

Wait for command completion. The /testgen command will:
- Analyze code to test
- Generate comprehensive test suite
- Create unit tests, integration tests
- Save tests to appropriate test directory

**VALIDATE TEST GENERATION OUTPUT:**

Check quality gates:
- âœ“ Test files created successfully?
- âœ“ Tests can be executed (no import/syntax errors)?
- âœ“ Test coverage â‰¥ 70%?
- âœ“ At least 80% of tests passing on first run?

Calculate validation score (0-100):
- Test files created: 25 points
- Tests executable: 25 points
- Coverage â‰¥ 70%: 25 points
- â‰¥ 80% tests passing: 25 points

To validate:
- Check test files exist in expected locations
- Try running tests with Bash (if test command available)
- Calculate pass rate from test output
- Estimate coverage from test file analysis

**IF VALIDATION SCORE >= 75:** Mark as âœ“ Passed, proceed to Step 5.4

**IF VALIDATION SCORE < 75 AND ATTEMPT < 3:**

**ATTEMPT 2 (Refined Strategy):**

Update status: "âš  Validation failed (score: [score]/100). Retrying with refined strategy... (Attempt 2/3)"

Analyze failure:
- If test creation failed: Check target path, ensure code to test exists
- If tests not executable: Fix import/syntax errors in generated tests
- If coverage low: Generate tests for more functions/methods
- If many tests failing: Fix implementation issues or test logic

Refined execution strategy:
- Fix syntax errors in generated tests using Edit tool
- Add missing test cases for uncovered code paths
- Fix failing tests by correcting assertions or test data
- Ensure test file structure matches project conventions
- Add setup/teardown if tests need initialization

Execute: `/testgen [target-path]` again OR manually fix tests with Edit tool

**VALIDATE AGAIN**

**IF STILL FAILS AND ATTEMPT < 3:**

**ATTEMPT 3 (Fallback Strategy):**

Update status: "âš  Attempt 2 failed. Using fallback strategy... (Attempt 3/3)"

Fallback approach:
- Generate minimal test suite (basic happy path tests only)
- Create test skeleton with TODOs for edge cases
- Accept lower coverage threshold (â‰¥50%)
- Create tests that pass even if coverage is low
- Add note: "âš  Minimal test suite - expand coverage recommended"

**VALIDATE AGAIN** (with lower threshold - 60% acceptable for fallback)

**IF STILL FAILS AFTER ATTEMPT 3:**
- Mark as "âš  NEEDS REVIEW - Test generation incomplete"
- Log failure: "Could not generate passing tests after 3 attempts"
- Provide what tests were generated
- Recommend manual test creation
- Continue workflow

### Step 5.4: Analyze Validated Test Results

Capture test generation results with validation data:
- Test files created (list with paths)
- Test cases generated (count)
- Coverage percentage
- Test frameworks used
- Tests passing vs failing
- Validation metrics

Store in structured format:

```
Testing Results:
- Command Executed: /testgen
- Attempts Required: [1-3]
- Validation Score: [0-100]
- Final Status: [âœ“ Passed / âš  Needs Review / âœ— Failed]
- Retry Strategy Used: [None / Refined / Fallback]
- Target: [path]
- Test Files Created: [count] - [list]
- Test Cases Generated: [count]
- Coverage: [percentage]%
- Framework(s): [list]
- Passing: [count]/[total] ([percentage]%)
- Failing: [count] - [list if any]
- Limitations: [any noted from fallback]
- Duration: [total time across all attempts]
- Validation Details:
  * Quality Gates Passed: [X/4]
  * Test Fixes Applied: [count]
  * Retry Actions Taken: [description if retried]
```

### Step 5.5: Test Status Assessment

Based on final test results:

**If all tests pass (or â‰¥80% passing):**
- Mark as "âœ“ Complete - Tests passing"
- High confidence in implementation quality

**If 50-79% tests passing:**
- Mark as "âš  Partial Success - Some tests failing"
- Log failures for manual review
- Continue workflow

**If <50% tests passing:**
- Mark as "âš  Needs Review - Many tests failing"
- Implementation may have issues
- Recommend reviewing implementation before proceeding

Update execution status:
- Mark /testgen with appropriate status
- Update commands completed count
- Update total retry attempts count
- If skipped: Mark as "âŠ˜ Skipped"

---

## Wave 6: Quality Assurance Phase

### Step 6.1: Determine Quality Commands Needed

Based on what hasn't been done yet and task type:

**Check Refactoring Need:**
- If task type is "Quality Improvement" â†’ YES (if not done in Wave 4)
- If code was implemented in Wave 4 â†’ YES (ensure quality)
- If task type is "Feature Development" â†’ YES (cleanup)
- If task type is "Bug Fix" â†’ MAYBE (if fixes were hacky)
- If task type is "Performance" â†’ MAYBE (if code is messy)
- Otherwise â†’ NO

**Check Optimization Need:**
- If task type is "Performance Optimization" â†’ YES (if not done in Wave 4)
- If task keywords include "slow", "faster", "optimize" â†’ YES
- If task type is "Comprehensive" â†’ YES
- If performance issues found in Wave 3 â†’ YES
- Otherwise â†’ MAYBE (for new features)

### Step 6.2: Execute Refactoring (with Validation Loop)

If refactoring is needed:

Initialize attempt tracking:
```
Refactoring Phase Tracking:
- Command: /refactor
- Attempt: 1
- Max Attempts: 3
- Status: Starting
```

**ATTEMPT 1 (Initial Execution):**

Update status: "â³ Step 5/N: Running /refactor [target] (Attempt 1/3)..."

Execute: `/refactor [target-path]`

Wait for command completion. The /refactor command will:
- Analyze code structure and quality
- Apply refactoring patterns
- Improve readability and maintainability
- Fix code smells

**VALIDATE REFACTORING OUTPUT:**

Check quality gates:
- âœ“ Files refactored successfully?
- âœ“ Code quality metrics improved or maintained?
- âœ“ No functionality broken (if tests available, they still pass)?
- âœ“ Refactoring patterns applied correctly?

Calculate validation score (0-100):
- Files refactored successfully: 30 points
- Quality improved/maintained: 30 points
- Functionality preserved: 25 points
- Patterns applied correctly: 15 points

To validate:
- Check files were modified
- Run tests if available (expect same pass rate)
- Check for code smell reduction
- Verify refactoring patterns make sense

**IF VALIDATION SCORE >= 75:** Mark as âœ“ Passed, proceed to Step 6.3

**IF VALIDATION SCORE < 75 AND ATTEMPT < 3:**

**ATTEMPT 2 (Refined Strategy):**

Update status: "âš  Validation failed (score: [score]/100). Retrying with refined strategy... (Attempt 2/3)"

Analyze failure:
- If refactoring broke functionality: Revert changes, apply more conservative refactoring
- If quality didn't improve: Try different refactoring patterns
- If patterns misapplied: Review code structure, apply appropriate patterns
- If files not modified: Broaden scope or adjust refactoring criteria

Refined execution strategy:
- Use more conservative refactoring approach
- Fix any broken tests using Edit tool
- Apply smaller, incremental refactorings
- Focus on high-impact refactorings only
- Verify after each refactoring step

Execute: `/refactor [target-path]` with refined approach OR manually apply refactorings with Edit tool

**VALIDATE AGAIN**

**IF STILL FAILS AND ATTEMPT < 3:**

**ATTEMPT 3 (Fallback Strategy):**

Update status: "âš  Attempt 2 failed. Using fallback strategy... (Attempt 3/3)"

Fallback approach:
- Apply minimal safe refactorings only (rename variables, extract constants)
- Skip complex structural refactorings
- Focus on code formatting and style consistency
- Add TODO comments for manual refactoring
- Note: "âš  Minimal refactoring - manual review recommended"

**VALIDATE AGAIN** (with lower threshold - 60% acceptable for fallback)

**IF STILL FAILS AFTER ATTEMPT 3:**
- Mark as "âš  NEEDS REVIEW - Refactoring incomplete"
- Log failure: "Could not safely refactor after 3 attempts"
- Note what was attempted
- Continue workflow

Capture refactoring results with validation data:

Store in structured format:

```
Refactoring Results:
- Command Executed: /refactor
- Attempts Required: [1-3]
- Validation Score: [0-100]
- Final Status: [âœ“ Passed / âš  Needs Review / âœ— Failed]
- Retry Strategy Used: [None / Refined / Fallback]
- Target: [path]
- Files Modified: [count] - [list]
- Patterns Applied: [list]
- Code Smells Fixed: [count] - [types]
- Complexity Reduced: [description]
- Readability Improved: [description]
- Tests Status: [All Passing / Some Failing / Not Run]
- Limitations: [any noted from fallback]
- Duration: [total time across all attempts]
- Validation Details:
  * Quality Gates Passed: [X/4]
  * Retry Actions Taken: [description if retried]
```

### Step 6.3: Execute Optimization (with Validation Loop)

If optimization is needed:

Initialize attempt tracking:
```
Optimization Phase Tracking:
- Command: /optimize
- Attempt: 1
- Max Attempts: 3
- Status: Starting
```

**ATTEMPT 1 (Initial Execution):**

Update status: "â³ Step 6/N: Running /optimize [target] (Attempt 1/3)..."

Execute: `/optimize [target-path]`

Wait for command completion. The /optimize command will:
- Profile performance bottlenecks
- Apply optimization techniques
- Improve algorithmic efficiency
- Reduce resource usage

**VALIDATE OPTIMIZATION OUTPUT:**

Check quality gates:
- âœ“ Optimizations applied successfully?
- âœ“ Performance improved or bottlenecks identified?
- âœ“ Functionality preserved (tests still pass if available)?
- âœ“ No new bugs introduced?

Calculate validation score (0-100):
- Optimizations applied: 25 points
- Performance improved: 30 points
- Functionality preserved: 30 points
- No new bugs: 15 points

To validate:
- Check files were modified with optimization intent
- Run tests if available (expect same pass rate)
- Look for performance improvements in code analysis
- Verify no obvious regressions

**IF VALIDATION SCORE >= 75:** Mark as âœ“ Passed, proceed to Step 6.4

**IF VALIDATION SCORE < 75 AND ATTEMPT < 3:**

**ATTEMPT 2 (Refined Strategy):**

Update status: "âš  Validation failed (score: [score]/100). Retrying with refined strategy... (Attempt 2/3)"

Analyze failure:
- If optimization broke functionality: Revert changes, try less aggressive optimization
- If no performance gain: Try different optimization techniques
- If tests failing: Fix broken code using Edit tool
- If new bugs introduced: Revert problematic optimizations

Refined execution strategy:
- Use more conservative optimization approach
- Profile code to identify actual bottlenecks
- Apply optimizations incrementally
- Test after each optimization
- Focus on algorithmic improvements over micro-optimizations

Execute: `/optimize [target-path]` with refined approach OR manually apply optimizations with Edit tool

**VALIDATE AGAIN**

**IF STILL FAILS AND ATTEMPT < 3:**

**ATTEMPT 3 (Fallback Strategy):**

Update status: "âš  Attempt 2 failed. Using fallback strategy... (Attempt 3/3)"

Fallback approach:
- Apply safe, proven optimizations only (e.g., caching, memoization)
- Add TODO comments noting optimization opportunities
- Document bottlenecks without modifying code
- Note: "âš  Minimal optimization - manual performance tuning recommended"

**VALIDATE AGAIN** (with lower threshold - 60% acceptable for fallback)

**IF STILL FAILS AFTER ATTEMPT 3:**
- Mark as "âš  NEEDS REVIEW - Optimization incomplete"
- Log failure: "Could not safely optimize after 3 attempts"
- Document identified bottlenecks
- Continue workflow

Capture optimization results with validation data:

Store in structured format:

```
Optimization Results:
- Command Executed: /optimize
- Attempts Required: [1-3]
- Validation Score: [0-100]
- Final Status: [âœ“ Passed / âš  Needs Review / âœ— Failed]
- Retry Strategy Used: [None / Refined / Fallback]
- Target: [path]
- Files Modified: [count] - [list]
- Optimizations Applied: [list]
- Performance Improvement: [percentage]% (or N/A)
- Before/After Metrics: [description]
- Techniques Used: [list]
- Tests Status: [All Passing / Some Failing / Not Run]
- Bottlenecks Identified: [list]
- Limitations: [any noted from fallback]
- Duration: [total time across all attempts]
- Validation Details:
  * Quality Gates Passed: [X/4]
  * Retry Actions Taken: [description if retried]
```

### Step 6.4: Final Bug Verification

If significant changes were made (implementation/refactoring/optimization):

Update status: "â³ Step 7/N: Running final /bughunter verification..."

Execute: `/bughunter [target-path]`

This is a verification scan to ensure:
- No new bugs were introduced
- Previous bugs were actually fixed
- Code quality maintained or improved

Compare with Wave 3 bug report (if available):
- Bugs fixed: [count]
- New bugs introduced: [count] (should be 0!)
- Net improvement: [+/- count]

Store in structured format:

```
Final Verification Results:
- Command Executed: /bughunter (verification)
- Status: [Success/Failed]
- Target: [path]
- Bugs Found: [count]
- Compared to Initial Scan: [improvement]
- New Bugs: [count] - [list if any]
- Fixed Bugs: [count]
- Status: [Clean/Needs Attention]
- Duration: [time]
```

Update execution status for all quality commands:
- Mark each as "âœ“ Complete", "âŠ˜ Skipped", or "âœ— Failed"
- Update commands completed count

---

## Wave 7: Documentation & Knowledge Phase

### Step 7.1: Determine Documentation Need

Documentation is valuable for ALL task types:
- **Feature Development** â†’ YES: Document new features
- **Bug Fix** â†’ YES: Document what was fixed and why
- **Quality Improvement** â†’ YES: Document improvements
- **Performance Optimization** â†’ YES: Document optimizations
- **Learning/Onboarding** â†’ YES: Core objective
- **Comprehensive** â†’ YES: Always document

Only skip if user explicitly says "no docs" in task.

### Step 7.2: Determine Documentation Scope

Based on task type and what was accomplished:

**For Feature Development:**
- Document new features and APIs
- Create usage examples
- Update README if needed
- Generate inline code comments

**For Bug Fixes:**
- Document what was broken
- Explain the fix
- Add comments to prevent regression
- Update troubleshooting docs

**For Quality/Performance:**
- Document improvements made
- Explain new patterns used
- Update architecture docs
- Add performance notes

**For Learning:**
- Generate comprehensive explanation docs
- Create onboarding guides
- Document architecture and design decisions

### Step 7.3: Execute Documentation (with Validation Loop)

Initialize attempt tracking:
```
Documentation Phase Tracking:
- Command: /document
- Attempt: 1
- Max Attempts: 3
- Status: Starting
```

**ATTEMPT 1 (Initial Execution):**

Update status: "â³ Step 8/N: Running /document [target] (Attempt 1/3)..."

Execute: `/document [target-path]`

Wait for command completion. The /document command will:
- Analyze code and changes
- Generate/update documentation files
- Add inline comments
- Create README/API docs as needed

**VALIDATE DOCUMENTATION OUTPUT:**

Check quality gates:
- âœ“ Documentation files created/updated?
- âœ“ Documentation coverage â‰¥ 60% of target?
- âœ“ Documentation is clear and accurate?
- âœ“ Follows project documentation standards?

Calculate validation score (0-100):
- Files created/updated: 25 points
- Coverage â‰¥ 60%: 30 points
- Clear and accurate: 25 points
- Follows standards: 20 points

To validate:
- Check documentation files exist
- Estimate coverage by comparing documented vs. undocumented elements
- Read sample sections for clarity and accuracy
- Check for consistent formatting and style

**IF VALIDATION SCORE >= 75:** Mark as âœ“ Passed, proceed to Step 7.4

**IF VALIDATION SCORE < 75 AND ATTEMPT < 3:**

**ATTEMPT 2 (Refined Strategy):**

Update status: "âš  Validation failed (score: [score]/100). Retrying with refined strategy... (Attempt 2/3)"

Analyze failure:
- If files not created: Check target path, ensure code to document exists
- If coverage low: Generate docs for more functions/modules
- If documentation unclear: Improve explanations, add examples
- If standards not followed: Apply project-specific doc templates

Refined execution strategy:
- Expand documentation scope to cover more code
- Improve documentation quality with better descriptions
- Add usage examples and code snippets
- Ensure consistent formatting
- Follow project documentation conventions

Execute: `/document [target-path]` again OR manually add documentation with Edit tool

**VALIDATE AGAIN**

**IF STILL FAILS AND ATTEMPT < 3:**

**ATTEMPT 3 (Fallback Strategy):**

Update status: "âš  Attempt 2 failed. Using fallback strategy... (Attempt 3/3)"

Fallback approach:
- Generate minimal documentation (function signatures, basic descriptions)
- Create documentation skeletons with placeholders
- Accept lower coverage threshold (â‰¥40%)
- Add TODO comments for sections needing expansion
- Note: "âš  Basic documentation - expand recommended"

**VALIDATE AGAIN** (with lower threshold - 60% acceptable for fallback)

**IF STILL FAILS AFTER ATTEMPT 3:**
- Mark as "âš  NEEDS REVIEW - Documentation incomplete"
- Log failure: "Could not generate adequate documentation after 3 attempts"
- Provide what documentation was generated
- Recommend manual documentation effort
- Continue workflow

### Step 7.4: Capture Validated Documentation Results

After documentation completion, capture with validation data:
- Documentation files created/updated (list)
- Inline comments added (count)
- README sections updated (list)
- API documentation generated (yes/no)
- Documentation coverage percentage
- Validation metrics

Store in structured format:

```
Documentation Results:
- Command Executed: /document
- Attempts Required: [1-3]
- Validation Score: [0-100]
- Final Status: [âœ“ Passed / âš  Needs Review / âœ— Failed]
- Retry Strategy Used: [None / Refined / Fallback]
- Target: [path]
- Files Created/Updated: [count] - [list]
- Inline Comments Added: [count]
- README Updated: [yes/no] - [sections]
- API Docs Generated: [yes/no]
- Coverage: [percentage]%
- Documentation Quality: [Good / Fair / Needs Work]
- Limitations: [any noted from fallback]
- Duration: [total time across all attempts]
- Validation Details:
  * Quality Gates Passed: [X/4]
  * Coverage Improvement: [percentage]
  * Retry Actions Taken: [description if retried]
```

### Step 7.5: Knowledge Artifacts

For learning tasks, ensure these artifacts exist:
- Architecture overview document
- Component interaction diagrams (if applicable)
- Onboarding guide
- FAQ section
- Code walkthrough

Update execution status:
- Mark /document as "âœ“ Complete" or "âœ— Failed"
- Update commands completed count

---

## Wave 8: Consolidation & Reporting

### Step 8.1: Aggregate All Results

Collect all stored results from Waves 2-7:
- Research findings
- Bug analysis
- Implementation details
- Test results
- Refactoring improvements
- Optimization gains
- Documentation coverage

### Step 8.2: Calculate Quality Metrics

Compute metrics based on available data:

**Test Coverage Metrics:**
- Coverage before: [estimate from task context or 0% for new]
- Coverage after: [from Wave 5 results]
- Coverage improvement: [after - before]

**Bug Metrics:**
- Bugs found initially: [from Wave 3]
- Bugs after verification: [from Wave 6]
- Bugs fixed: [initial - final]
- Net improvement: [percentage]

**Performance Metrics (if applicable):**
- Performance before: [from /optimize or N/A]
- Performance after: [from /optimize]
- Improvement percentage: [calculated]

**Code Quality Metrics:**
- Code smells before: [from initial /bughunter]
- Code smells after: [from final /bughunter]
- Quality score change: [improvement]

**Documentation Metrics:**
- Documentation coverage before: [estimate]
- Documentation coverage after: [from Wave 7]
- Improvement: [percentage]

### Step 8.3: Execution Summary Statistics

Calculate workflow statistics including validation metrics:
- Total commands executed: [count]
- Commands successful: [count]
- Commands failed: [count]
- Commands skipped: [count]
- Commands needing review: [count]
- **Total retry attempts across all commands**: [count]
- **Average attempts per command**: [total attempts / commands executed]
- Total duration: [end time - start time]
- Average time per command: [total / count]
- **Commands that required retries**: [list]
- **Commands that passed on first attempt**: [count]

### Step 8.4: Determine Overall Status

Based on results, determine overall status:

**âœ“ SUCCESS:**
- All critical commands completed successfully
- Primary objective achieved
- No critical failures
- Tests passing (if generated)

**âš  PARTIAL SUCCESS:**
- Primary objective achieved but with issues
- Some commands failed but workflow continued
- Tests have failures
- Some quality issues remain

**âœ— FAILED:**
- Primary objective not achieved
- Critical commands failed
- Implementation has serious bugs
- Cannot proceed without manual intervention

### Step 8.5: Generate Comprehensive Report

Create the final report using this exact format:

```markdown
# Workflow Execution Report

## Task
{{ARGUMENTS}}

## Execution Summary

**Task Type**: [Primary classification] [+ Secondary if hybrid]
**Commands Executed**: [successful]/[total] successful
**Total Duration**: [X minutes Y seconds]
**Overall Status**: [âœ“ SUCCESS / âš  PARTIAL SUCCESS / âœ— FAILED]
**Total Retry Attempts**: [count] across [X] commands
**First-Attempt Success Rate**: [percentage]%

## Workflow Steps

[For each command that was executed, include a section like this:]

### [N]. [Command Name] ([/command])
**Status**: [âœ“ Complete / âš  Needs Review / âœ— Failed / âŠ˜ Skipped]
**Attempts**: [1-3] (Initial [+ Refined] [+ Fallback])
**Validation Score**: [0-100]/100
**Duration**: [time across all attempts]
**Retry Strategy**: [None / Refined / Fallback]
**Output**: [Key output description]
**Key Findings**:
- [Bullet point 1]
- [Bullet point 2]
- [Bullet point 3]
**Validation Notes**: [Any quality gates failed, limitations noted, etc.]

[Repeat for each command in execution order]

## Validation Summary

**Commands by Validation Status**:
- âœ“ Passed First Attempt: [count] commands
- ðŸ”„ Required Refinement (2 attempts): [count] commands
- ðŸ”„ðŸ”„ Required Fallback (3 attempts): [count] commands
- âš  Needs Review (all attempts exhausted): [count] commands

**Quality Gate Statistics**:
| Command | Score | Gates Passed | Attempts | Strategy |
|---------|-------|--------------|----------|----------|
| /research | [X]/100 | [Y]/4 | [1-3] | [strategy] |
| /build | [X]/100 | [Y]/4 | [1-3] | [strategy] |
| /testgen | [X]/100 | [Y]/4 | [1-3] | [strategy] |
[... continue for each executed command]

## Quality Metrics

| Metric | Before | After | Change |
|--------|--------|-------|--------|
| Test Coverage | [X]% | [Y]% | [+/-Z]% [â†‘/â†“] |
| Code Quality | [grade/score] | [grade/score] | [change] [â†‘/â†“] |
| Performance | [time/metric] | [time/metric] | [%] [â†‘/â†“] |
| Bugs Found | [count] | [count] | [change] [â†‘/â†“] |
| Documentation | [X]% | [Y]% | [+/-Z]% [â†‘/â†“] |

[Note: Include only metrics that are relevant and have data]

## Files Changed

### Files Created ([count])
[List files with absolute paths]

### Files Modified ([count])
[List files with absolute paths]

### Files Deleted ([count] if any)
[List files with absolute paths]

## Command Outputs

[For each command executed, provide detailed output:]

### [Command Name] Output
[Include the most important findings, results, or content from this command]
[Use code blocks, lists, or paragraphs as appropriate]

## Issues & Resolutions

[If any errors or issues occurred:]

### Issue 1: [Description]
**Command**: [command that failed]
**Error**: [error message]
**Resolution**: [how it was handled]
**Impact**: [impact on workflow]

[Repeat for each issue]

[If no issues:]
No significant issues encountered during execution.

## Next Steps

Based on the workflow results, here are recommended next steps:

1. [Specific actionable recommendation]
2. [Specific actionable recommendation]
3. [Specific actionable recommendation]
[Continue as needed]

## Recommendations

### Short-term
- [Recommendation for immediate action]
- [Recommendation for immediate action]

### Long-term
- [Recommendation for future improvements]
- [Recommendation for maintenance]

### Best Practices
- [Suggestion for ongoing quality]
- [Suggestion for team processes]

## Workflow Efficiency

**Time Investment**: [total time]
**Manual Effort Saved**: [estimate of time saved vs manual]
**Efficiency Gain**: [X]x faster than manual execution
**Commands Reusable**: [list commands that can be run independently]

## Conclusion

[2-3 sentence summary of what was accomplished, the overall quality of results, and the state of the codebase after workflow completion]
```

### Step 8.6: Present the Report

Output the complete report to the user.

End with a clear statement:
```
---
Workflow execution completed. The [task-type] task has been [completed/partially completed/failed].
[If success:] All objectives achieved successfully.
[If partial:] Primary objectives achieved with [X] issues noted above.
[If failed:] Unable to complete due to [primary reason]. Manual intervention required.
```

---

## Error Handling & Recovery

### Critical Error Handling

If a command fails, follow this protocol:

**1. Capture Error Details:**
- Command that failed
- Error message
- Stack trace (if available)
- Context (what was being attempted)

**2. Assess Impact:**
- Is this command blocking for the workflow?
- Can subsequent commands still execute?
- Is partial success still valuable?

**3. Determine Action:**

**If command is CRITICAL (blocking):**
- Abort workflow gracefully
- Generate partial results report
- Clearly state what was accomplished and what couldn't be done
- Provide manual recovery steps

**If command is IMPORTANT (but not blocking):**
- Log the error
- Mark command as "âœ— Failed"
- Skip dependent commands
- Continue with independent commands
- Note in final report

**If command is OPTIONAL:**
- Log the error
- Mark command as "âœ— Failed"
- Continue workflow normally
- Note in final report

**4. Dependency Handling:**

Track command dependencies:
- /build depends on /research (for features)
- /testgen depends on /build (for new code)
- /document depends on implementation commands

If a command fails, automatically skip dependent commands and mark them as "âŠ˜ Skipped - Dependency failed"

### Invalid Task Handling

If the task is invalid or unclear:

```markdown
# Invalid Task Error

**Task Provided**: {{ARGUMENTS}}

**Issue**: [Describe what's wrong - too vague, contradictory, missing info, etc.]

**Suggested Actions**:
1. Clarify your objective: [specific question]
2. Specify target path: [what to specify]
3. Provide context: [what context is needed]

**Usage Examples**:

Feature Development:
/workflow implement user authentication with JWT tokens

Bug Fixing:
/workflow fix memory leak in src/components/dashboard.ts

Code Quality:
/workflow improve code quality in src/services/

Performance:
/workflow optimize API response times in the payment service

Learning:
/workflow explain how the authentication system works

Comprehensive:
/workflow complete overhaul of the user management system

Please provide a clearer task description and try again.
```

### Command Timeout Handling

If a command takes too long (>10 minutes):
- Display progress indicator
- Allow command to continue
- Note in report that command took longer than expected
- Suggest optimization for future runs

### Resource Limitation Handling

If running low on context or resources:
- Prioritize most important commands
- Skip optional optimization/documentation if needed
- Generate abbreviated report
- Note limitations in report

---

## Intelligent Features

### Feature 1: Task Classification Intelligence

Use multiple indicators for classification:
- **Keyword matching**: Primary method (as defined in Wave 1)
- **Context analysis**: Look at mentioned files, components, technologies
- **Scope indicators**: Words like "entire", "all", "complete" suggest comprehensive
- **Urgency indicators**: Words like "urgent", "critical", "broken" suggest bug fix priority

### Feature 2: Adaptive Execution

Adjust workflow based on discoveries:
- Start in Standard mode but upgrade to Comprehensive if needed
- Downgrade to Quick mode if task simpler than expected
- Re-route commands if research reveals different approach
- Add commands if new needs discovered

### Feature 3: Progress Visualization

Throughout execution, show current status:

```
Workflow Progress: [====================    ] 80% (6/8 commands)

Current: â³ Running /testgen src/features/auth/
Completed: âœ“ Research âœ“ Build âœ“ BugHunter âœ“ Refactor âœ“ Optimize
Pending: â¸ Document â¸ Final Report

Estimated time remaining: 3 minutes
```

### Feature 4: Smart Command Chaining

Pass output from one command to another:
- /research output (plan file) â†’ /build input
- /bughunter output (bug list) â†’ /build input (for fixes)
- /build output (files created) â†’ /testgen input
- /testgen output (test results) â†’ Quality assessment
- All outputs â†’ /document input (for comprehensive docs)

### Feature 5: Result Caching

Store intermediate results for potential reuse:
- Research plans
- Bug reports
- Test results
- Refactoring patterns applied

If workflow is run again on same target, offer to reuse previous results where applicable.

### Feature 6: Parallel Execution Detection

Identify commands that COULD run in parallel (for future optimization):
- /refactor and /optimize can sometimes run in parallel
- /testgen and /document can run in parallel
- Multiple /explain commands can run in parallel

Note these opportunities in the report for future enhancement.

---

## Advanced Patterns

### Pattern: Iterative Refinement

For complex tasks, offer iterative refinement:
1. Run initial workflow
2. Analyze results
3. Identify gaps
4. Run targeted follow-up workflow
5. Repeat until quality threshold met

### Pattern: Multi-Target Workflow

If task mentions multiple targets:
- Parse all targets from task
- Run workflow on each target sequentially
- Consolidate results into single report
- Highlight differences between targets

Example: "workflow improve quality in src/services/ and src/utils/"

### Pattern: Staged Rollout

For comprehensive tasks, suggest staged approach:
1. Phase 1: Core implementation + tests
2. Phase 2: Quality improvements
3. Phase 3: Performance optimization
4. Phase 4: Documentation

### Pattern: Verification Loops

For critical tasks (security, data integrity):
- Run /bughunter before and after
- Run tests multiple times
- Verify no regressions
- Extra /explain to verify understanding

---

## Success Criteria

A workflow execution is considered successful if:

1. **Primary Objective Achieved**: The user's main task is accomplished
2. **Quality Threshold Met**: Tests pass, no critical bugs remain
3. **Documentation Complete**: Changes are documented appropriately
4. **No Regressions**: Existing functionality not broken
5. **Performance Maintained**: No significant performance degradation (unless optimizing)
6. **Comprehensive Report**: User understands what was done and results

---

## Execution Checklist

Before starting each wave:
- [ ] Understand current state
- [ ] Determine commands needed
- [ ] Check dependencies satisfied
- [ ] Update progress display

During each command:
- [ ] Display status update
- [ ] Execute via SlashCommand tool
- [ ] Wait for completion
- [ ] Capture results
- [ ] Handle errors if any

After each command:
- [ ] Update execution state
- [ ] Mark command status
- [ ] Store results
- [ ] Check if workflow should continue

Before final report:
- [ ] All commands completed or skipped
- [ ] All results captured
- [ ] Metrics calculated
- [ ] Issues documented
- [ ] Recommendations prepared

---

## Final Notes

**You are the conductor of an orchestra of powerful commands.** Your job is to:
1. Understand what the user wants (Wave 1)
2. Execute commands in optimal order (Waves 2-7)
3. Synthesize results into actionable insights (Wave 8)

**Remember:**
- Be intelligent about command selection
- Be adaptive when discoveries change the plan
- Be thorough in capturing and reporting results
- Be clear about successes, failures, and partial results
- Be helpful with next steps and recommendations

**The goal:** Make the user feel like they have a senior developer who can take a high-level task and execute it end-to-end with minimal supervision, while keeping them informed throughout the process.

---

## Quality Gate Framework (Iterative Validation)

This workflow includes **iterative validation loops** where each command validates its output against quality criteria. If validation fails, the command retries with an improved strategy (max 3 attempts). This ensures high-quality results rather than simply executing once and moving on.

### Quality Gate Principles

1. **Self-Assessment**: Each command evaluates its own output
2. **Adaptive Retry**: Failed attempts retry with refined strategies
3. **Max Attempts**: Limit to 3 attempts per command (initial + 2 retries)
4. **Graceful Degradation**: If max retries exceeded, flag but continue workflow
5. **Transparent Reporting**: Track and report all retry attempts

### Validation Criteria by Command

**Quality Gates for /research:**
- âœ“ Found at least 3 relevant files (any type: source, docs, config, tests)
- âœ“ Generated implementation plan successfully
- âœ“ Plan contains actionable tasks and clear structure
- âœ“ Plan file saved to `.claude/plans/` directory
- âœ— Retry triggers: 0 files found, plan generation failed, plan is too vague

**Quality Gates for /explain:**
- âœ“ Successfully analyzed target code/path
- âœ“ Identified key components and dependencies
- âœ“ Generated clear explanation of functionality
- âœ“ No analysis errors or crashes
- âœ— Retry triggers: Analysis failed, target not found, explanation too shallow

**Quality Gates for /bughunter:**
- âœ“ Successfully analyzed target files
- âœ“ Generated bug report (even if 0 bugs found is valid)
- âœ“ Report saved to `.claude/bugs/` directory
- âœ“ Categorized bugs by severity
- âœ— Retry triggers: Analysis crashed, no report generated, couldn't read files

**Quality Gates for /build:**
- âœ“ All planned files created/modified successfully
- âœ“ No syntax errors in generated code
- âœ“ Code follows project conventions
- âœ“ Implementation matches plan requirements
- âœ— Retry triggers: File write failed, syntax errors, missing critical components

**Quality Gates for /testgen:**
- âœ“ Test files generated successfully
- âœ“ Tests can be executed (no import/syntax errors)
- âœ“ Test coverage > 70% (or explain why lower)
- âœ“ At least 80% of tests passing on first run
- âœ— Retry triggers: Test generation failed, all tests failing, coverage < 50%

**Quality Gates for /refactor:**
- âœ“ Files refactored successfully
- âœ“ Code quality metrics improved or maintained
- âœ“ No functionality broken (if tests available, they still pass)
- âœ“ Refactoring patterns applied correctly
- âœ— Retry triggers: Refactoring broke code, quality degraded, patterns misapplied

**Quality Gates for /optimize:**
- âœ“ Optimizations applied successfully
- âœ“ Performance improved or identified bottlenecks
- âœ“ Functionality preserved (tests still pass if available)
- âœ“ No new bugs introduced
- âœ— Retry triggers: Optimization broke functionality, no improvements found

**Quality Gates for /document:**
- âœ“ Documentation files created/updated
- âœ“ Documentation coverage > 60% of target
- âœ“ Documentation is clear and accurate
- âœ“ Follows project documentation standards
- âœ— Retry triggers: Doc generation failed, coverage < 30%, docs are inaccurate

### Retry Strategy Framework

Each command follows this retry progression:

**Attempt 1 (Initial Execution):**
- Execute with original parameters from workflow plan
- Use standard scope and default settings
- Expected: 85% success rate

**Attempt 2 (Refined Strategy) - If Attempt 1 fails validation:**
- Analyze failure reason
- Apply refinement strategy:
  - **Broader scope**: Search more locations, use wildcards, include more file types
  - **Alternative methods**: Try different search patterns, different analysis techniques
  - **Relaxed constraints**: Adjust thresholds, accept partial results
  - **Enhanced context**: Use more information from previous waves
- Expected: 90% success rate (cumulative 98.5%)

**Attempt 3 (Fallback Strategy) - If Attempt 2 fails validation:**
- Use fallback/minimal approach:
  - **Minimal acceptable output**: Generate basic/skeleton version
  - **Manual placeholder**: Create structure for human completion
  - **Best effort with limitations**: Proceed with what's available
  - **Explicit limitations noted**: Document what couldn't be done
- Expected: 95% success rate (cumulative 99.9%)

**If Attempt 3 fails validation:**
- Flag command as "âš  NEEDS REVIEW"
- Log detailed failure information
- Continue workflow (don't block unless critical dependency)
- Include in final report with recommendations

### Validation Loop Template

Each wave follows this validation loop structure:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step N.1: Execute Command           â”‚
â”‚ - Run slash command with parameters â”‚
â”‚ - Capture raw output                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step N.2: Validate Output           â”‚
â”‚ - Check against quality gates       â”‚
â”‚ - Calculate validation score        â”‚
â”‚ - Identify specific failures        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
        [Quality Gate Met?]
              â†“
        YES â†“     â†“ NO
            â†“     â†“
            â†“     â””â”€â†’ [Attempt < 3?]
            â†“              â†“
            â†“         YES â†“    â†“ NO
            â†“             â†“    â†“
            â†“   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â†’ [Flag as Needs Review]
            â†“   â†“                           â†“
            â†“   Step N.3: Analyze Failure   â†“
            â†“   - Determine failure reason  â†“
            â†“   - Select retry strategy     â†“
            â†“   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“          â†“
            â†“   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â†“   â”‚ Step N.4: Retry Executionâ”‚
            â†“   â”‚ - Apply refined strategy â”‚
            â†“   â”‚ - Increment attempt countâ”‚
            â†“   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“          â”‚
            â†“          â””â”€â†’ [Loop back to Step N.1]
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step N.5: Store Validated Resultsâ”‚
â”‚ - Record final output             â”‚
â”‚ - Log attempt count               â”‚
â”‚ - Note any limitations            â”‚
â”‚ - Proceed to next wave            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Execution State Tracking (Enhanced)

Track detailed state including validation attempts:

```
Workflow Execution State:
- Start Time: [timestamp]
- Total Commands: [count]
- Commands Completed: 0
- Commands Failed: 0
- Commands Skipped: 0
- Commands Needing Review: 0
- Total Retry Attempts: 0
- Current Phase: Planning Complete
- Overall Status: Starting

Per-Command Tracking:
[command-name]:
  - Attempts: [1-3]
  - Validation Score: [0-100]
  - Status: [âœ“ Passed / âš  Needs Review / âœ— Failed / âŠ˜ Skipped]
  - Retry Strategy Used: [None / Refined / Fallback]
  - Failure Reasons: [list if any]
  - Duration: [total time across all attempts]
```

---

## Wave 9: Update Organization Files & Final Summary

**Purpose:** Update TODO.md, STATUS.md, and ROADMAP.md with complete workflow results and generate integrated summary.

### Step 9.1: Update TODO.md (if exists)

Mark workflow task as completed and add follow-up tasks:

```markdown
## âœ… Completed Recently
- [x] [Task description] (Completed: [timestamp])
  - Workflow: [mode] mode - [N] commands executed
  - Duration: [total duration]
  - Status: [âœ“ SUCCESS / âš  PARTIAL SUCCESS]
  - Commands: [successful]/[total] successful
  - Validation: [first-attempt success rate]%

## ðŸ“‹ Up Next
[Add new tasks discovered during workflow:]
- [ ] [Next step from recommendations]
- [ ] [Another next step]
- [ ] [Address any items marked âš  Needs Review]
```

### Step 9.2: Update STATUS.md (if exists)

Remove from active workflows and add to completed work:

```markdown
## ðŸ”„ Active Workflows
[Remove this workflow entry]

## âœ… Workflows Completed
- **[Task description]** - Completed [date]
  - Mode: [Quick/Standard/Comprehensive]
  - Duration: [total time]
  - Commands Executed: [successful]/[total]
  - Validation Scores: [average score]/100
  - Retry Rate: [X]% required retries
  - First-Attempt Success: [Y]%
  - Quality Metrics:
    * Test Coverage: [before]% â†’ [after]% ([change])
    * Bugs: [before] â†’ [after] ([fixed] fixed)
    * Documentation: [before]% â†’ [after]% ([change])
  - Files Changed: [created/modified counts]
  - Status: [âœ“ All objectives met / âš  Some issues remain]
```

### Step 9.3: Update ROADMAP.md (if exists)

Check if workflow completed any phase objectives:

**If phase objectives were completed:**
```markdown
### Phase [N]: [Phase Name]
**Progress:** [X/Y] ([old%]) â†’ [(X+completed)/Y] ([new%])  â† Updated!
- [x] [Objective that was completed] (Completed: [date])  â† Marked complete!
  - Via: /workflow [task description]
  - Results: [brief summary]
```

**Update phase progress percentage** if objectives completed.

### Step 9.4: Generate Integrated Final Summary

Enhance the Wave 8 report with organization file context:

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ“ WORKFLOW EXECUTION COMPLETE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ“‹ TASK: [Task description]

â± EXECUTION SUMMARY:
   â€¢ Mode: [Quick/Standard/Comprehensive]
   â€¢ Duration: [total time]
   â€¢ Commands: [successful]/[total] successful
   â€¢ Status: [âœ“ SUCCESS / âš  PARTIAL SUCCESS / âœ— FAILED]

ðŸŽ¯ VALIDATION METRICS:
   â€¢ Total Retry Attempts: [count] across [X] commands
   â€¢ First-Attempt Success: [percentage]%
   â€¢ Average Validation Score: [score]/100
   â€¢ Commands Passed First Try: [count]
   â€¢ Commands Needed Retries: [count]
   â€¢ Commands Needing Review: [count]

ðŸ“Š QUALITY IMPROVEMENTS:
   â€¢ Test Coverage: [before]% â†’ [after]% ([+/-change]%)
   â€¢ Bugs Fixed: [count] ([initial] â†’ [final])
   â€¢ Code Quality: [improvement description]
   â€¢ Documentation: [before]% â†’ [after]% ([+/-change]%)
   â€¢ Performance: [improvement if applicable]

ðŸ“ FILES CHANGED:
   â€¢ Created: [count] files
   â€¢ Modified: [count] files
   â€¢ Total Changes: [lines added/modified]

ðŸ“‹ ORGANIZATION FILES UPDATED:
   â€¢ TODO.md: Task completed, [X] new tasks added
   â€¢ STATUS.md: Workflow results recorded
   â€¢ ROADMAP.md: [Phase progress updated: X% â†’ Y% / No phase impact]

ðŸŽ¯ STRATEGIC ALIGNMENT:
   â€¢ Current Phase: [phase from ROADMAP]
   â€¢ Phase Progress: [X%] â†’ [Y%]
   â€¢ Objectives Completed: [list if any]
   â€¢ Contributing To: [strategic goal from ROADMAP]
   â€¢ Next Milestone: [from ROADMAP]

ðŸ“ COMMANDS EXECUTED:
[For each command:]
   [N]. /[command] - [Status]
       â€¢ Attempts: [1-3]
       â€¢ Score: [validation score]/100
       â€¢ Strategy: [None/Refined/Fallback]
       â€¢ Duration: [time]
       â€¢ Key Output: [brief summary]

ðŸš€ RECOMMENDED NEXT STEPS:
   1. [Next step from recommendations]
   2. [Another next step]
   3. [Follow-up based on workflow results]

âš ï¸ ITEMS NEEDING REVIEW:
[If any commands failed or need review:]
   â€¢ [Command] - [Issue description]
   â€¢ [Another item if applicable]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[Continue with detailed Wave 8 report as specified in Step 8.5]
```

**Note:** If organization files don't exist, omit the organization file sections and provide standard Wave 8 report only.

---

Now, execute the workflow for the task: **{{ARGUMENTS}}**

Begin with Wave 0: Context & Organization Check, then proceed to Wave 1: Task Analysis & Workflow Planning.
